---
layout:     post
title:      "自动驾驶中的场景理解"
subtitle:   "Fast Scene Understanding for Autonomous Driving"
date:       2018-06-03 17:00:00
author:     "donkey"
header-img: "img/post/blog-2018.jpg"
catalog: true
tags:
    - 论文
    - autonomous driving
    - perception
---

## 论文简介
* 论文地址：[Fast Scene Understanding for Autonomous Driving](https://arxiv.org/abs/1708.02550)
* 演示视频地址：[https://youtu.be/55ElRh-g_7o](https://youtu.be/55ElRh-g_7o)
* 论文基于ENet，给出了一种实时的解决方案，可以同时解决语义场景分割、实例分割以及单目深度估计这三个自动驾驶过程中的重要问题。使用共享的encoder做初步的特征提取，针对3个不同的任务，使用不同的decoder。
* 可以在Cityscapes数据集上跑出21fps的速度(1024X512)，单个任务的精度也非常高。
* 论文没有介绍新的网络架构或者模型，但是融合了已有的很多东西，给出了一种实时场景理解的方法。
* 论文中提到，在模型的精度性能没有大幅下降的情况下，提升模型计算的速度对于自动驾驶是更加重要的，现在有的很多模型由于采用了神经网络的方法，参数量巨大，很难做到实时。

## 论文解决的三个问题
### 语义分割
* loss function是标准的pixel-wise的cross-entropy loss

### 实例分割
* 使用前面blog中提到的`Discriminative Loss Function`作为loss function

### 基于单目的深度估计
* 之前比较经典的方法是使用L2的loss作为loss function，论文中使用`Eigen和Fergus`提出的`reverse Huber loss`，模型的性能比使用L2 loss有所提升.

## 其他
* ENet论文：[https://arxiv.org/pdf/1606.02147.pdf](https://arxiv.org/pdf/1606.02147.pdf)，是一个可以用于语义分割的FCN。